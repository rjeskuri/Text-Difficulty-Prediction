{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stable-fifteen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "offensive-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('.git/text-difficulty-prediction/dataset/WikiLarge_Train.csv')\n",
    "test_df = pd.read_csv('.git/text-difficulty-prediction/dataset/WikiLarge_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gorgeous-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df['original_text'], train_df['label'], test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "employed-local",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    177224\n",
       "1    177028\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making sure class balance is still okay\n",
    "class_counts = y_train.value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "statutory-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up tf-idf for machine learning on just training set\n",
    "#then set up using features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "numeric-testimony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8521      Diego María de la Concepción Juan Nepomuceno E...\n",
       "182810    Some of the 1930s trams are still in regular s...\n",
       "275464                Emperor Go-Momozono -LRB- Japan -RRB-\n",
       "176814    In other countries , potassium iodate is used ...\n",
       "196293    Located in a region called Planalto Central , ...\n",
       "Name: original_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extreme-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "#need to provide a series or dataframe\n",
    "def text_clean(text):\n",
    "    #Tokenize\n",
    "    tokens = text.apply(word_tokenize)\n",
    "    #Stem\n",
    "    stemmed = tokens.apply(text_stemmer)\n",
    "    #rejoin from list to sentence\n",
    "    cleaned_text = stemmed.apply(\" \".join)\n",
    "    return cleaned_text\n",
    "def text_stemmer(token_text):\n",
    "    stemmed_words = [stemmer.stem(word) for word in token_text]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accredited-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_clean = text_clean(X_val)\n",
    "X_train_clean = text_clean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "constitutional-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',min_df = 20, max_df = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legendary-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Now we convert the cleaned text to a TF-IDF representation\n",
    "# vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X_train_clean)\n",
    "X_val = vectorizer.transform(X_val_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defensive-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions that do the following -- tokenize, stem, , vectorizer mindf maxdf stopword removal\n",
    "# split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comprehensive-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(random_state=42).fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prepared-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mathematical-occurrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6786902552946446\n",
      "Precision: 0.6861844496579982\n",
      "Recall: 0.662265595101416\n",
      "F1 Score: 0.6740128856359241\n",
      "ROC AUC Score: 0.6787419117997452\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_val, preds)\n",
    "precision = precision_score(y_val, preds)\n",
    "recall = recall_score(y_val, preds)\n",
    "f1 = f1_score(y_val, preds)\n",
    "roc_auc = roc_auc_score(y_val, preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "potential-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #parameter tuning\n",
    "# model = LogisticRegression()\n",
    "# param_grid = {\n",
    "#     'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "#     'penalty': ['l1', 'l2'],\n",
    "#     'solver': ['liblinear', 'saga']\n",
    "# }\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "# # Fit the GridSearchCV object to the data\n",
    "# grid_search.fit(X,y_train)\n",
    "# print('Best parameters:', grid_search.best_params_)\n",
    "# preds = grid_search.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amino-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = accuracy_score(y_val, preds)\n",
    "# precision = precision_score(y_val, preds)\n",
    "# recall = recall_score(y_val, preds)\n",
    "# f1 = f1_score(y_val, preds)\n",
    "# roc_auc = roc_auc_score(y_val, preds)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {f1}')\n",
    "# print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "rf_model = RandomForestClassifier()\n",
    "# gb_model = GradientBoostingClassifier()\n",
    "# svm_model = svm.SVC()\n",
    "\n",
    "# Fit the models\n",
    "rf_model.fit(X, y_train)\n",
    "# gb_model.fit(X, y_train)\n",
    "# svm_model.fit(X, y_train)\n",
    "\n",
    "# Make predictions on validation for now \n",
    "rf_predictions = rf_model.predict(X_val)\n",
    "# gb_predictions = gb_model.predict(X_val)\n",
    "# svm_predictions = svm_model.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_f1 = f1_score(y_val, rf_predictions)\n",
    "# gb_f1 = f1_score(y_val, gb_predictions)\n",
    "# svm_f1 = f1_score(y_val, svm_predictions)\n",
    "\n",
    "print(f'F1 Score for random forest: {rf_f1}')\n",
    "# print(f'F1 Score for gradient boosted forest: {gb_f1}')\n",
    "# print(f'F1 Score for SVM: {svm_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dress-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_aoa = pd.read_csv('.git/text-difficulty-prediction/features/AoA_51715_words.csv')\n",
    "features_concrete = pd.read_csv('.git/text-difficulty-prediction/features/Concreteness_ratings_Brysbaert_et_al_BRM.txt', delimiter = '\\t')\n",
    "features_dale_chall = pd.read_csv('.git/text-difficulty-prediction/features/dale_chall.txt',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "contemporary-cologne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Alternative.spelling</th>\n",
       "      <th>Freq_pm</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Nletters</th>\n",
       "      <th>Nphon</th>\n",
       "      <th>Nsyll</th>\n",
       "      <th>Lemma_highest_PoS</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>Perc_known</th>\n",
       "      <th>AoA_Kup_lem</th>\n",
       "      <th>Perc_known_lem</th>\n",
       "      <th>AoA_Bird_lem</th>\n",
       "      <th>AoA_Bristol_lem</th>\n",
       "      <th>AoA_Cort_lem</th>\n",
       "      <th>AoA_Schock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>20415.27</td>\n",
       "      <td>Article</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>aardvark</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacus</td>\n",
       "      <td>abacus</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Noun</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>abacus</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abacuses</td>\n",
       "      <td>abacuses</td>\n",
       "      <td>0.02</td>\n",
       "      <td>Noun</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>abacus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abalone</td>\n",
       "      <td>abalone</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Verb</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>abalone</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>12.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Alternative.spelling   Freq_pm Dom_PoS_SUBTLEX  Nletters  Nphon  \\\n",
       "0         a                    a  20415.27         Article         1      1   \n",
       "1  aardvark             aardvark      0.41            Noun         8      7   \n",
       "2    abacus               abacus      0.24            Noun         6      6   \n",
       "3  abacuses             abacuses      0.02            Noun         8      9   \n",
       "4   abalone              abalone      0.51            Verb         7      7   \n",
       "\n",
       "   Nsyll Lemma_highest_PoS  AoA_Kup  Perc_known  AoA_Kup_lem  Perc_known_lem  \\\n",
       "0      1                 a     2.89        1.00         2.89            1.00   \n",
       "1      2          aardvark     9.89        1.00         9.89            1.00   \n",
       "2      3            abacus     8.69        0.65         8.69            0.65   \n",
       "3      4            abacus      NaN         NaN         8.69            0.65   \n",
       "4      4           abalone    12.23        0.72        12.23            0.72   \n",
       "\n",
       "   AoA_Bird_lem  AoA_Bristol_lem  AoA_Cort_lem  AoA_Schock  \n",
       "0          3.16              NaN           NaN         NaN  \n",
       "1           NaN              NaN           NaN         NaN  \n",
       "2           NaN              NaN           NaN         NaN  \n",
       "3           NaN              NaN           NaN         NaN  \n",
       "4           NaN              NaN           NaN         NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This file contains \"Age of Acquisition\" (AoA) estimates for about 51k English words, which refers to the approximate age (in years) when a word was learned. Early words, being more basic, have lower average AoA.\n",
    "\n",
    "The main columns you will be interested in are \"Word\" and \"AoA_Kup_lem\". But the others may be useful too.\n",
    "\n",
    "Word :: The word in question\n",
    "Alternative.spelling :: if the Word may be spelled frequently in another form\t\n",
    "Freq_pm\t:: Freq of the Word in general English (larger -> more common)\n",
    "Dom_PoS_SUBTLEX\t:: Dominant part of speech in general usage\n",
    "Nletters :: number of letters \n",
    "Nphon :: number of phonemes\n",
    "Nsyll :: number of syllables\n",
    "Lemma_highest_PoS :: the \"lemmatized\" or \"root\" form of the word (in the dominant part of speech. e.g. The root form of the verb \"abates\" is \"abate\".\n",
    "AoA_Kup\t:: The AoA from a previous study by Kuperman et al.\n",
    "Perc_known :: Percent of people who knew the word in the Kuperman et al. study\n",
    "AoA_Kup_lem :: Estimated AoA based on Kuperman et al. study lemmatized words. THIS IS THE MAIN COLUMN OF INTEREST.\n",
    "Perc_known_lem\t:: Estimated percentage of people who would know this form of the word in the Kuperman study.\n",
    "AoA_Bird_lem :: AoA reported in previous study by Bird (2001) \n",
    "AoA_Bristol_lem\t:: AoA reported in previous study from Bristol Univ. (2006)\n",
    "AoA_Cort_lem :: AoA reported in previous study by Cortese & Khanna (2008)\n",
    "AoA_Schock :: AoA reported in previous study by Schock (2012)\n",
    "\n",
    "\"\"\"\n",
    "features_aoa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "equipped-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Conc.SD</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent_known</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>Dom_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roadsweeper</td>\n",
       "      <td>0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traindriver</td>\n",
       "      <td>0</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tush</td>\n",
       "      <td>0</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0.88</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hairdress</td>\n",
       "      <td>0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pharmaceutics</td>\n",
       "      <td>0</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Bigram  Conc.M  Conc.SD  Unknown  Total  Percent_known  \\\n",
       "0    roadsweeper       0    4.85     0.37        1     27           0.96   \n",
       "1    traindriver       0    4.54     0.71        3     29           0.90   \n",
       "2           tush       0    4.45     1.01        3     25           0.88   \n",
       "3      hairdress       0    3.93     1.28        0     29           1.00   \n",
       "4  pharmaceutics       0    3.77     1.41        4     26           0.85   \n",
       "\n",
       "   SUBTLEX Dom_Pos  \n",
       "0        0       0  \n",
       "1        0       0  \n",
       "2       66       0  \n",
       "3        1       0  \n",
       "4        0       0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file contains concreteness ratings for 40 thousand English lemma words gathered via Amazon Mechanical Turk. The ratings come from a larger list of 63 thousand words and represent all English words known to 85% of the raters.\n",
    "\n",
    "The file contains eight columns:\n",
    "1. The word\n",
    "2. Whether it is a single word or a two-word expression \n",
    "3. The mean concreteness rating\n",
    "4. The standard deviation of the concreteness ratings\n",
    "5. The number of persons indicating they did not know the word\n",
    "6. The total number of persons who rated the word\n",
    "7. Percentage participants who knew the word\n",
    "8. The SUBTLEX-US frequency count (on a total of 51 million; Brysbaert & New, 2009) \n",
    "9. The dominant part-of-speech usage\n",
    "\n",
    "\"\"\"\n",
    "features_concrete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "israeli-commerce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>able</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         a\n",
       "0     able\n",
       "1   aboard\n",
       "2    about\n",
       "3   above \n",
       "4  absent "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the Dale Chall 3000 Word List, which is one definition of words that are considered \"basic\" English.\n",
    "A summary is at https://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php\n",
    "\n",
    "\"\"\"\n",
    "features_dale_chall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-corps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
